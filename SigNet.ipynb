{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SigNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SravanthiKavala/Convolution/blob/master/SigNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV6GkYyYEheG",
        "colab_type": "code",
        "outputId": "54d1cece-eed5-4de9-e1bb-b3205912eeba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "! git clone https://github.com/SravanthiKavala/SigNetProject\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SigNetProject'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "Unpacking objects:   1% (1/64)   \rUnpacking objects:   3% (2/64)   \rUnpacking objects:   4% (3/64)   \rUnpacking objects:   6% (4/64)   \rUnpacking objects:   7% (5/64)   \rUnpacking objects:   9% (6/64)   \rUnpacking objects:  10% (7/64)   \rUnpacking objects:  12% (8/64)   \rUnpacking objects:  14% (9/64)   \rUnpacking objects:  15% (10/64)   \rUnpacking objects:  17% (11/64)   \rUnpacking objects:  18% (12/64)   \rUnpacking objects:  20% (13/64)   \rUnpacking objects:  21% (14/64)   \rUnpacking objects:  23% (15/64)   \rUnpacking objects:  25% (16/64)   \rUnpacking objects:  26% (17/64)   \rUnpacking objects:  28% (18/64)   \rUnpacking objects:  29% (19/64)   \rUnpacking objects:  31% (20/64)   \rUnpacking objects:  32% (21/64)   \rUnpacking objects:  34% (22/64)   \rUnpacking objects:  35% (23/64)   \rUnpacking objects:  37% (24/64)   \rUnpacking objects:  39% (25/64)   \rUnpacking objects:  40% (26/64)   \rUnpacking objects:  42% (27/64)   \rUnpacking objects:  43% (28/64)   \rUnpacking objects:  45% (29/64)   \rUnpacking objects:  46% (30/64)   \rUnpacking objects:  48% (31/64)   \rUnpacking objects:  50% (32/64)   \rUnpacking objects:  51% (33/64)   \rUnpacking objects:  53% (34/64)   \rUnpacking objects:  54% (35/64)   \rUnpacking objects:  56% (36/64)   \rUnpacking objects:  57% (37/64)   \rUnpacking objects:  59% (38/64)   \rUnpacking objects:  60% (39/64)   \rUnpacking objects:  62% (40/64)   \rUnpacking objects:  64% (41/64)   \rUnpacking objects:  65% (42/64)   \rUnpacking objects:  67% (43/64)   \rUnpacking objects:  68% (44/64)   \rUnpacking objects:  70% (45/64)   \rUnpacking objects:  71% (46/64)   \rUnpacking objects:  73% (47/64)   \rUnpacking objects:  75% (48/64)   \rUnpacking objects:  76% (49/64)   \rUnpacking objects:  78% (50/64)   \rremote: Total 64 (delta 0), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects:  79% (51/64)   \rUnpacking objects:  81% (52/64)   \rUnpacking objects:  82% (53/64)   \rUnpacking objects:  84% (54/64)   \rUnpacking objects:  85% (55/64)   \rUnpacking objects:  87% (56/64)   \rUnpacking objects:  89% (57/64)   \rUnpacking objects:  90% (58/64)   \rUnpacking objects:  92% (59/64)   \rUnpacking objects:  93% (60/64)   \rUnpacking objects:  95% (61/64)   \rUnpacking objects:  96% (62/64)   \rUnpacking objects:  98% (63/64)   \rUnpacking objects: 100% (64/64)   \rUnpacking objects: 100% (64/64), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJWjjZ2LTU0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13cacef5-5c49-4c19-aad8-602c5bf55d3d"
      },
      "source": [
        "!cd SigNet\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: SigNet: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlyajdNnTute",
        "colab_type": "code",
        "outputId": "912c50bd-997e-4518-cd04-44cf73a40752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  SigNetProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkogPenEEh8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load SigNetProject/SignatureDataGenerator.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syj0CL4veYyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64315185-c05e-47e1-a7ba-4ae4a2ccfada"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  SigNetProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBoXz7SxD_P8",
        "colab_type": "code",
        "outputId": "99b1c5d2-ec9c-46e5-b7e7-95e1ae0633cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "#from keras.utils.visualize_util import plot\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras.layers import Dense, Dropout, Input, Lambda, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.preprocessing import image\n",
        "from keras import backend as K\n",
        "from SigNetProject import SignatureDataGenerator\n",
        "import getpass as gp\n",
        "import sys\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD, RMSprop, Adadelta\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "import random\n",
        "random.seed(1337)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBFv7BK9243C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEnUSQf93U_r",
        "colab": {}
      },
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7M3_CfM3XcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5-mG7T73cjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD7Mqgew3eqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_base_network_signet(input_shape):\n",
        "    \n",
        "    seq = Sequential()\n",
        "    seq.add(Convolution2D(96, 11, 11, activation='relu', name='conv1_1', subsample=(4, 4), input_shape= input_shape, \n",
        "                        init='glorot_uniform', dim_ordering='tf'))\n",
        "    seq.add(BatchNormalization(epsilon=1e-06, mode=0, axis=1, momentum=0.9))\n",
        "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))    \n",
        "    seq.add(ZeroPadding2D((2, 2), dim_ordering='tf'))\n",
        "    \n",
        "    seq.add(Convolution2D(256, 5, 5, activation='relu', name='conv2_1', subsample=(1, 1), init='glorot_uniform',  dim_ordering='tf'))\n",
        "    seq.add(BatchNormalization(epsilon=1e-06, mode=0, axis=1, momentum=0.9))\n",
        "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
        "    seq.add(Dropout(0.3))# added extra\n",
        "    seq.add(ZeroPadding2D((1, 1), dim_ordering='tf'))\n",
        "    \n",
        "    seq.add(Convolution2D(384, 3, 3, activation='relu', name='conv3_1', subsample=(1, 1), init='glorot_uniform',  dim_ordering='tf'))\n",
        "    seq.add(ZeroPadding2D((1, 1), dim_ordering='tf'))\n",
        "    \n",
        "    seq.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2', subsample=(1, 1), init='glorot_uniform', dim_ordering='tf'))    \n",
        "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
        "    seq.add(Dropout(0.3))# added extra\n",
        "#    model.add(SpatialPyramidPooling([1, 2, 4]))\n",
        "    seq.add(Flatten(name='flatten'))\n",
        "    seq.add(Dense(1024, W_regularizer=l2(0.0005), activation='relu', init='glorot_uniform'))\n",
        "    seq.add(Dropout(0.5))\n",
        "    \n",
        "    seq.add(Dense(128, W_regularizer=l2(0.0005), activation='relu', init='glorot_uniform')) # softmax changed to relu\n",
        "    \n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXO9EZ2R3nNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy_roc(predictions, labels):\n",
        "   '''Compute ROC accuracy with a range of thresholds on distances.\n",
        "   '''\n",
        "   dmax = np.max(predictions)\n",
        "   dmin = np.min(predictions)\n",
        "   nsame = np.sum(labels == 1)\n",
        "   ndiff = np.sum(labels == 0)\n",
        "   \n",
        "   step = 0.01\n",
        "   max_acc = 0\n",
        "   \n",
        "   for d in np.arange(dmin, dmax+step, step):\n",
        "       idx1 = predictions.ravel() <= d\n",
        "       idx2 = predictions.ravel() > d\n",
        "       \n",
        "       tpr = float(np.sum(labels[idx1] == 1)) / nsame       \n",
        "       tnr = float(np.sum(labels[idx2] == 0)) / ndiff\n",
        "       acc = 0.5 * (tpr + tnr)       \n",
        "#       print ('ROC', acc, tpr, tnr)\n",
        "       \n",
        "       if (acc > max_acc):\n",
        "           max_acc = acc\n",
        "           \n",
        "   return max_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDAPTwOUKcQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6ddb558b-f707-432c-f2d5-cfec8cd4c30b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVDImbf0Tneh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/GPDS300/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUO93k0W5Mvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "483f7d52-eb0e-4bfc-e05c-d460e3212374"
      },
      "source": [
        "def read_signature_data(dataset, ntuples, height = 30, width = 100):\n",
        "    \n",
        "    usr = gp.getuser()\n",
        "\n",
        "#    image_dir = '/home/' + usr + '/Workspace/SignatureVerification/Datasets/' + dataset + '/'\n",
        " #   image_dir = '/home/' + usr + '/Workspace/Datasets/' + dataset + '/'\n",
        "    data_file = root_path + '_pairs.txt'\n",
        "    \n",
        "    f = open( data_file, 'r' )\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "    \n",
        "    \n",
        "    idx = np.random.choice(list(range(len(lines))), ntuples)\n",
        "    \n",
        "    lines = [lines[i] for i in idx]\n",
        "    \n",
        "    images = []\n",
        "    \n",
        "    for line in lines:\n",
        "        file1, file2, label = line.split(' ')\n",
        "                                       \n",
        "        img1 = image.load_img(image_dir + file1, grayscale = True, \n",
        "                target_size=(height, width))\n",
        "                \n",
        "        img1 = image.img_to_array(img1)#, dim_ordering='tf')\n",
        "                \n",
        "        images.append(img1)\n",
        "        \n",
        "        img2 = image.load_img(image_dir + file1, grayscale = True, \n",
        "                target_size=(height, width))\n",
        "            \n",
        "        img2 = image.img_to_array(img2)#, dim_ordering='tf')\n",
        "                \n",
        "        images.append(img2)\n",
        "        \n",
        "    return np.array(images)\n",
        "        \n",
        "def main(args):\n",
        "    dataset = args.dataset\n",
        "   # if dataset == 'Bengali':\n",
        "    \n",
        "    #    tot_writers = 100\n",
        "     #   num_train_writers = 80\n",
        "      #  num_valid_writers = 10\n",
        "        \n",
        "   # elif dataset == 'Hindi':\n",
        "    #    \n",
        "     #   tot_writers = 160\n",
        "      #  num_train_writers = 100\n",
        "       # num_valid_writers = 10\n",
        "        \n",
        "    if dataset == 'GPDS300':\n",
        "    \n",
        "        tot_writers = 300\n",
        "        num_train_writers = 240\n",
        "        num_valid_writers = 30\n",
        "        \n",
        "   # elif dataset == 'GPDS960':\n",
        "    #\n",
        "     #   tot_writers = 4000\n",
        "      #  num_train_writers = 3200\n",
        "       # num_valid_writers = 400\n",
        "        \n",
        "   # elif dataset == 'CEDAR1':\n",
        "    #\n",
        "     #   tot_writers = 55\n",
        "      #  num_train_writers = 45\n",
        "       # num_valid_writers = 5\n",
        "    \n",
        "    num_test_writers = tot_writers - (num_train_writers + num_valid_writers)\n",
        "    \n",
        "    # parameters\n",
        "    batch_sz = args.batch_size #128\n",
        "    nsamples = args.num_samples #276 \n",
        "    img_height = 155\n",
        "    img_width = 220\n",
        "    featurewise_center = False\n",
        "    featurewise_std_normalization = True\n",
        "    zca_whitening = False\n",
        "    nb_epoch = args.epoch #20    \n",
        "    input_shape=(img_height, img_width, 1)\n",
        "\n",
        "    # initialize data generator   \n",
        "    datagen = SignatureDataGenerator(dataset, tot_writers, num_train_writers, \n",
        "        num_valid_writers, num_test_writers, nsamples, batch_sz, img_height, img_width,\n",
        "        featurewise_center, featurewise_std_normalization, zca_whitening)\n",
        "    \n",
        "    # data fit for std\n",
        "    X_sample = read_signature_data(dataset, int(0.5*tot_writers), height=img_height, width=img_width)\n",
        "    datagen.fit(X_sample)\n",
        "    del X_sample\n",
        "    \n",
        "    # network definition\n",
        "    base_network = create_base_network_signet(input_shape)\n",
        "    \n",
        "    input_a = Input(shape=(input_shape))\n",
        "    input_b = Input(shape=(input_shape))\n",
        "    \n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "    \n",
        "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "    \n",
        "    model = Model(input=[input_a, input_b], output=distance)\n",
        "    \n",
        "    # compile model\n",
        "    rms = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08)\n",
        "    adadelta = Adadelta()\n",
        "    model.compile(loss=contrastive_loss, optimizer=rms)\n",
        "    \n",
        "    # display model\n",
        "#    plot(model, show_shapes=True)\n",
        "#    sys.exit()     \n",
        "    \n",
        "    # callbacks\n",
        "   #SRAVANTHIIII\n",
        "  #fname = os.path.join('/home/sounak/Documents/' , 'weights_'+str(dataset)+'.hdf5')\n",
        "    fname = str(root_path)+'/weights_GPDS300.hdf5'\n",
        "    checkpointer = ModelCheckpoint(filepath=fname, verbose=1, save_best_only=True)\n",
        "#    tbpointer = TensorBoard(log_dir='/home/adutta/Desktop/Graph', histogram_freq=0,  \n",
        "#          write_graph=True, write_images=True)\n",
        "    #print int(datagen.samples_per_valid)\n",
        "    # print datagen.samples_per_train\n",
        "    # print int(datagen.samples_per_valid)\n",
        "    # print int(datagen.samples_per_test)\n",
        "    # sys.exit()\n",
        "    # train model   \n",
        "    model.fit_generator(generator=datagen.next_train(), samples_per_epoch=datagen.samples_per_train, nb_epoch=nb_epoch,\n",
        "                        validation_data=datagen.next_valid(), nb_val_samples=int(datagen.samples_per_valid))   # KERAS 1\n",
        "    model.fit_generator(generator=datagen.next_train(), steps_per_epoch=960, epochs=nb_epoch,\n",
        "                        validation_data=datagen.next_valid(), validation_steps=120, callbacks=[checkpointer])  # KERAS 2\n",
        "    # load the best weights for test\n",
        "    model.load_weights(fname)\n",
        "    print (fname)\n",
        "    print ('Loading the best weights for testing done...') \n",
        "   \n",
        "#    tr_pred = model.predict_generator(generator=datagen.next_train(), val_samples=int(datagen.samples_per_train))\n",
        "    te_pred = model.predict_generator(generator=datagen.next_test(), steps=120)\n",
        "    \n",
        "#    tr_acc = compute_accuracy_roc(tr_pred, datagen.train_labels)\n",
        "    te_acc = compute_accuracy_roc(te_pred, datagen.test_labels)\n",
        "    \n",
        "#    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "    print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n",
        "    \n",
        "# Main Function    \n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Signature Verification')\n",
        "    # required training parameters\n",
        "    parser.add_argument('--dataset', '-ds', action='store', type=str, required=True,\n",
        "                  help='Please mention the database.')\n",
        "    # required tensorflow parameters\n",
        "    parser.add_argument('--epoch', '-e', action='store', type=int, default=20, \n",
        "                  help='The maximum number of iterations. Default: 20')\n",
        "    parser.add_argument('--num_samples', '-ns', action='store', type=int, default=276, \n",
        "                  help='The number of samples. Default: 276')\n",
        "    parser.add_argument('--batch_size', '-bs', action='store', type=int, default=138,\n",
        "                  help='The mini batch size. Default: 138')\n",
        "    args = parser.parse_args()\n",
        "    # print args.dataset, args.epoch, args.num_samples, args.batch_size\n",
        "#    sys.exit()\n",
        "    main(args)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] --dataset DATASET [--epoch EPOCH]\n",
            "                             [--num_samples NUM_SAMPLES]\n",
            "                             [--batch_size BATCH_SIZE]\n",
            "ipykernel_launcher.py: error: the following arguments are required: --dataset/-ds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}